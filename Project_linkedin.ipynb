{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_linkedin.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDceq2p2fB9r+k93t6HVJJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilbert83416/NTUT-Crawler-Project/blob/main/Project_linkedin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k9Kq7tdeSWiq"
      },
      "source": [
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials as sac\n",
        "from time import sleep\n",
        "\n",
        "def upload_data(tables,x):\n",
        "    auth_json = \"perfect-reducer-295008-ad85edadeaf1.json\"\n",
        "    gs_scopes = ['https://spreadsheets.google.com/feeds']\n",
        "    credit = sac.from_json_keyfile_name(auth_json, gs_scopes)\n",
        "    google_credit = gspread.authorize(credit)\n",
        "\n",
        "    spreadsheet_key = \"1_ErDWvrYcocaipHGhIyWYcZeA_EVFeZgAsFlhoZipBk\"\n",
        "    sheet = google_credit.open_by_key(spreadsheet_key)\n",
        "    # sheet = google_credit.open(\"linkedin_jobs\")\n",
        "    worksheet = sheet.sheet1\n",
        "    list_title = [\"ID\",\"Job\",\"Company\",\"Description\",\"Industry\",\"Location\",\"Status\",\"company web\"]\n",
        "    worksheet.append_row(list_title)\n",
        "\n",
        "    for i in range(0,x):\n",
        "        datas = [tables[\"ID\"][i],\n",
        "                 tables[\"Job\"][i],\n",
        "                 tables[\"Company\"][i],\n",
        "                 tables[\"Description\"][i],\n",
        "                 tables[\"Industry\"][i],\n",
        "                 tables[\"Location\"][i],\n",
        "                 tables[\"Status\"][i],\n",
        "                 tables[\"company web\"][i]]\n",
        "        worksheet.append_row(datas)\n",
        "        sleep(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW2g6ZNsSlpH"
      },
      "source": [
        "import requests\n",
        "\n",
        "def send_linkedin_jobs():\n",
        "    msg = \"https://docs.google.com/spreadsheets/d/1_ErDWvrYcocaipHGhIyWYcZeA_EVFeZgAsFlhoZipBk/edit?usp=sharing\"\n",
        "    token2 = \"yJkgsSruIJVnrjxmm1n0UJveciY5N3C5r5P5L2wXS0F\"\n",
        "    headers = {\n",
        "        \"Authorization\": \"Bearer \"+ token2,\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
        "    }\n",
        "    payload = {\"message\": msg}\n",
        "    notify = requests.post(\"https://notify-api.line.me/api/notify\", headers = headers, params = payload)\n",
        "    print(notify.status_code)\n",
        "    if notify.status_code == 200:\n",
        "        print(\"success\")\n",
        "    else:\n",
        "        print(\"failed\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWUWO-s0SsvQ"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import ssl\n",
        "ssl._create_default_https_context= ssl._create_unverified_context\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "tables = {\n",
        "\"ID\":[],\n",
        "\"Job\":[],\n",
        "\"Company\":[],\n",
        "\"Description\": [],\n",
        "\"Industry\":[],\n",
        "\"Location\":[],\n",
        "\"Status\":[],\n",
        "\"company web\":[],\n",
        "}\n",
        "\n",
        "headers = {\n",
        "'cookie': 'JSESSIONID=ajax:8261265241262329541; lang=v=2&lang=zh-tw; bcookie=\"v=2&b06d4326-bd9e-4e4f-8eec-3ce19d68b29b\"; bscookie=\"v=1&202102240741338a8bf5ea-0855-4552-8f53-d2095ca06b5eAQHHrmVvwPK95lSGFcwomHnAgzDqYcMi\"; lidc=\"b=OGST05:s=O:r=O:g=2137:u=1:i=1614152493:t=1614238893:v=1:sig=AQHRCvIOVAbyQ9g0NlQaN2-xRqbh_Ks3\"; _ga=GA1.2.1895089971.1614152495; _gid=GA1.2.186299571.1614152495; _gcl_au=1.1.628253537.1614152497; AMCVS_14215E3D5995C57C0A495C55%40AdobeOrg=1; AMCV_14215E3D5995C57C0A495C55%40AdobeOrg=-637568504%7CMCIDTS%7C18683%7CMCMID%7C53356341410453737313405019638203026464%7CMCAAMLH-1614757297%7C11%7CMCAAMB-1614757297%7C6G1ynYcLPuiQxYZrsz_pkqfLG9yMXBpb2zX5dvJdYQJzPXImdj0y%7CMCOPTOUT-1614159697s%7CNONE%7CvVersion%7C5.1.1; aam_uuid=53876661086840609043426712026391576555; recent_history=AQE9tefl-8rLFwAAAXfTDtaqAq8qV9_PltuHThEPEi1R02MCGMwxG3EftFi5Ep1dhoN4MGkp54rNgKS_yPeJp-xX0qqjUbGym-0_yY0jTOWZJ3loarpzNVXmDqlh-BD51kvjTcJSt--_cRxrgPxcvLnuWUac3q7H3OZfLA3veODTveO-7nHYj21gKTsPs4zLwnH8BiqvAXaN3-GbrgYvzPHMFTf5pFoFDUxnNkgSiSO1rNy8mMg9pLRBsgOMtrUECsan2lFy8BKAox8e6TfCRkQ2fcGKeYsyo5w4x8PFDBqoiSg2WTQqnRjE29_RAGE2IsNDg85CH-1Yewzdf7ND_vGLnEBatI8rZsCwKw',\n",
        "'csrf-token': 'ajax:8261265241262329541',\n",
        "'referer': 'https://www.linkedin.com/jobs/search/?geoId=104187078&keywords=data%20analyst&start=0&redirect=false&position=1&pageNum=0',\n",
        "'sec-ch-ua': '\"Chromium\";v=\"88\", \"Google Chrome\";v=\"88\", \";Not A Brand\";v=\"99\"',\n",
        "'sec-ch-ua-mobile': '?0',\n",
        "'sec-fetch-dest': 'empty',\n",
        "'sec-fetch-mode': 'cors',\n",
        "'sec-fetch-site': 'same-origin',\n",
        "'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36'\n",
        "}\n",
        "\n",
        "header2 = {\n",
        "'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36',\n",
        "}\n",
        "# Step 1 : give me url and return ID list\n",
        "ID = []\n",
        "for i in [0, 25 ,50, 75]:\n",
        "    url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?geoId=104187078&keywords=data%20analyst&start=\"\\\n",
        "          + str(i) +\"&redirect=false&position=1&pageNum=0\"\n",
        "    response = requests.get(url, headers = headers)\n",
        "    print(response)\n",
        "    html = BeautifulSoup(response.text)\n",
        "    # print(html)\n",
        "    print(\"page:\",i)\n",
        "    lists = html.find_all(\"li\", class_=\"job-result-card\")\n",
        "    for list in lists:\n",
        "        # print(list[\"data-id\"])\n",
        "        ID.append(list[\"data-id\"])\n",
        "print(ID)\n",
        "len_ID = len(ID)\n",
        "print(len_ID)\n",
        "j = 1\n",
        "# Step 2: give me ID list give you tables\n",
        "for i in ID:\n",
        "    url2 = \"https://tw.linkedin.com/jobs/view/data-analyst-taipei-at-gogolook-co-ltd-\"+ str(i)+\"?refId=c3de1abf-2e6f-4116-b8fc-c3422e376dde&trackingId=G9FnaoPODfKL%2FA%2BMQjkqcQ%3D%3D&position=1&pageNum=0&trk=public_jobs_job-result-card_result-card_full-click\"\n",
        "    # print(url2)\n",
        "    response2 = requests.get(url2, headers=header2)\n",
        "    # print(response2)\n",
        "    html = BeautifulSoup(response2.text)\n",
        "    print(html)\n",
        "    # details = json.loads(html.find(\"p\").text)\n",
        "    # print(details)\n",
        "    details = html.find(\"div\", class_=\"topcard__content-left\")\n",
        "    title = details.find(\"h1\", class_= \"topcard__title\") \n",
        "    company_name = html.find(\"span\", class_=\"topcard__flavor\")\n",
        "    location = html.find(\"span\", class_=\"topcard__flavor--bullet\")\n",
        "    company_description = html.find(\"div\", class_=\"show-more-less-html__markup\")\n",
        "    details = html.find_all(\"span\", class_=\"job-criteria__text\")\n",
        "    Employment_Status = details[1]\n",
        "    industries_len = len(html.find_all(\"li\", class_=\"job-criteria__item\"))\n",
        "    print(\"Job: \", title.text)\n",
        "    print(\"Company:\", company_name.text)\n",
        "    print(\"Description:\", company_description.text)\n",
        "    print(\"Location: \", location.text)\n",
        "    print(\"Status: \", Employment_Status.text)\n",
        "    industry = \"\"\n",
        "    industries = html.find_all(\"li\", class_=\"job-criteria__item\")[industries_len - 1].find_all(\"span\")\n",
        "    for ind in industries:\n",
        "        industry = industry + ind.text + \",\"\n",
        "    tables[\"Industry\"].append(industry)\n",
        "    print(\"Industry: \", industry)\n",
        "    tables[\"ID\"].append(i)\n",
        "    tables[\"Job\"].append(title.text)\n",
        "    tables[\"Company\"].append(company_name.text)\n",
        "    tables[\"Description\"].append(company_description.text.replace(\"\\n\", \" \"))\n",
        "    tables[\"Location\"].append(location.text)\n",
        "    tables[\"Status\"].append(Employment_Status.text)\n",
        "    try:\n",
        "        company_url = html.find_all(\"a\", class_=\"apply-button\")[0][\"href\"]\n",
        "        tables[\"company web\"].append(company_url)\n",
        "        print(\"company web\", company_url)\n",
        "    except IndexError:\n",
        "        tables[\"company web\"].append(\"None\")\n",
        "        print(\"company web: None\")\n",
        "\n",
        "    print(\"id\", i)\n",
        "    print(j, \"-\"*50)\n",
        "    j = j + 1\n",
        "\n",
        "df = pd.DataFrame(tables)\n",
        "# df.to_csv(\"Job_list_google.csv\", encoding=\"utf-8\")\n",
        "\n",
        "upload_data(tables,len_ID)\n",
        "send_linkedin_jobs()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}